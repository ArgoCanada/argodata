---
title: "Virtual moorings and sections"
---

```{r, include = FALSE}
library(argodata)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(ggspatial)
library(Hmisc)
theme_set(theme_bw())
argo_global_prof()
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

We'll start by loading a few packages.

```{r setup, eval=FALSE}
library(argodata)
library(tidyverse)
library(lubridate)
theme_set(theme_bw())
```

## Fetch

The first step is to choose which area is representative of the region for which you would like to create an aggregated/summarized set of profiles. One way to do this is using a point and radius, for which `argo_filter_radius()` is provided to subset the global profile index `argo_global_prof()`. For example, the following point/radius results in approximately 475 profiles between Labrador and Greenland in the Labrador Sea. For the purposes of the virtual mooring, the radius needs to be small enough that the profiles are related to the conditions in the region you are attempting to model and large enough that there are enough profiles for window point of time you would like to model.

```{r}
mooring_lat <- 55.7
mooring_lon <- -49.5
mooring_radius_km <- 150
```

The next step is to (optionally) choose a window of time to consider. For the purposes of this article, we'll consider the years between 2011 and 2019, inclusive.

```{r}
mooring_start <- as.Date("2011-01-01")
mooring_end <- as.Date("2019-12-31")
```

Finally, it is worth considering whether or not the "realtime" or "delayed" data sets best suits your use-case. The delayed data set is more likely to have poor-quality measurements corrected; however, the realtime data set may have more profiles available. You should pick one or the other to avoid double-counting profiles collected by the same float! In this article we will use the realtime data set.

An index of all profiles available from the Argo program is available by calling `argo_global_prof()`. This will take 20-60 seconds to load depending on your internet connection; if you would like to avoid downloading the index file more than once you can [configure a persistent cache directory](https://github.com/ArgoCanada/argodata#installation), but be aware that this index is updated frequently.

```{r}
profiles <- argo_global_prof() %>%
  argo_filter_radius(
    latitude = mooring_lat,
    longitude = mooring_lon,
    radius_km = mooring_radius_km
  ) %>% 
  argo_filter_date(
    date_min = mooring_start,
    date_max = mooring_end
  ) %>% 
  argo_filter_data_mode("realtime") %>% 
  argo_extract_path_info() %>% 
  select(file, file_float, date, latitude, longitude, everything()) %>% 
  arrange(date)

profiles
```

A good sanity check is to examine the distribution of profiles in space and time, as we'll be spending a lot of time examining the interactions between these dimensions. Below I've created a bar plot to examine the distribution of profiles both within and between years: there is a lot of variability! We will come back to this later on.

```{r}
ggplot(
  profiles,
  aes(
    x = factor(year(date)),
    fill = factor(month(date))
  )
) +
  geom_bar()
```


For a quick view of the locations I used the [ggspatial](https://github.com/paleolimbot/ggspatial) package:


```{r, warning=FALSE}
library(ggspatial)

ggplot(profiles, aes(x = longitude, y = latitude, col = )) +
  annotation_map_tile(zoomin = -1, progress = "none") +
  geom_spatial_rect(
    aes(ymin = 50, ymax = 65, xmin = -70, xmax = -30),
    fill = NA,
    # data = tibble(x = NA),
    inherit.aes = FALSE,
    crs = 4326
  ) +
  geom_spatial_point(crs = 4326) +
  facet_wrap(vars(year(date)))
```

You can use plots like this to ensure that you have a reasonable density of samples in both space and time for the question you are trying to answer. Once you've done this, you can load profile levels using `argo_prof_levels()`. This will download the files from the Argo server and load them into a table with one row per profile per sampling level. This will take about 90 seconds depending on your internet connection and system configuration (if you do this often, see `argo_set_mapper()` for how to use multiple cores on your computer to do this faster).

```{r}
levels <- argo_prof_levels(profiles) %>% 
  select(file, date, pres, temp, temp_qc, everything())

levels
```

## Clean

Again, the first step once we have the data is to plot! Here I've coloured the points by the `_qc` column for temperature, since temperature is what we'll be examining later on. There is a `_qc` column for most variables in the levels table; you can learn more about what each flag means in the `argo_reference_qc_flag` table or the [Argo User's Manual](https://doi.org/10.13155/29825).

```{r}
ggplot(levels, aes(y = pres, x = temp, col = temp_qc)) +
  geom_point() +
  scale_y_reverse()
```

From this plot it's clear that there are some points with clearly bad values that we need to take care of. A similar plot of `pres_qc` indicates that there are some bad pressure values as well. Depending what stage you are at in your analysis, you may want to remove rows that you can't use in future analysis or set these values to `NA`. I'll demonstrate the latter here using `argo_qc_censor_if_not()`, which sets values to `NA` where the paired `_qc` column is not in a specified vector of values. The only value that makes sense to keep based on a plot of our results is `1`, which corresponds to "good data" in the reference table (beware that not all data marked "good" has been checked with the same degree of scrutiny!).

```{r}
levels_clean <- levels %>%
  argo_qc_censor_if_not(c(temp, pres), qc_flag = 1)

ggplot(levels_clean, aes(y = pres, x = temp)) +
  geom_line(
    aes(group = interaction(file, n_prof)),
    orientation = "y",
    alpha = 0.05
  ) +
  scale_y_reverse()
```

We've set a lot of values to `NA` - in some cases many profiles worth. While we do want to keep a record of levels even if the temperature was set to `NA` because of its `_qc` column value, we also don't need profiles where there is little temperature information, and including them in our data moving forward is not useful.

```{r}
levels_clean <- levels_clean %>% 
  group_by(file, n_prof) %>% 
  filter(mean(is.finite(temp)) > 0.5) %>% 
  ungroup()
```


## Model

### Bin/Aggregate

Combining information collected at specific locations over time is an entire subfield of spatial statistics. For our purposes, binning and aggregating along a few dimensions of interest is probably sufficient and should always be attempted before invoking a more complex method.

There are a few complexities associated with combining information from multiple profiles. Notably, there is a severe sampling bias: the nature of the Argo float is such that it collects detailed data along its trajectory which is clumped in both space and time. This means that certain areas and timeframes are intensely sampled whereas other areas and/or timeframes may have poor coverage. Another complexity is that certain locations may represent the "virtual mooring" location or region poorly.

We will mitigate the effects of both these complicating behaviours using weighting. To ensure that a single float does not contribute unduly to any particular prediction, we can weight each profile as `1 / n`, where `n` is the number of profiles in a given time frame contributed by a single float. We can also weight profiles taken farther from the mooring location less than those taken close to the mooring location; however, the choice of how severely to punish profiles taken far from the mooring location makes a considerable difference to the result. One method is to use the inverse of the distance, which produces a rather severe punishment as distance from the centre increases. Here we will use `1 / sqrt(distance)` to mitigate the penalty. The distance weights we can calculate before binning; the float weights depend on our choice of bins.

```{r}
profile_weighted <- profiles %>%
  mutate(
    distance_km = s2::s2_distance(
      s2::s2_lnglat(longitude, latitude), 
      s2::s2_lnglat(mooring_lon, mooring_lat)
    ) / 1000,
    distance_weight = 1 / sqrt(distance_km)
  ) %>% 
  select(file, longitude, latitude, distance_km, distance_weight)

profile_weighted
```

The next step is to choose bins that are appropriate to the scale of the problem. In this case, we're interested in monthly bins (regardless of year) and depth bins 10 decibars in height. These bin sizes reflect the amount of data we have: if we were only summarizing 2019, for which there are many profiles with high-resolution sampling intervals, we may be able to use smaller bins (e.g., one bin per month per year or 1 decibar height).

```{r}
levels_binned <- levels_clean %>% 
  mutate(
    pres_bin = floor(pres / 100) * 100 + 50,
    date_bin = month(date)
  ) %>% 
  select(pres_bin, date_bin, everything())

levels_binned
```

Next we can calculate the weights to ensure each profile does not contribute more than its share to the values calculated for a given bin and apply the weights based on distance we calculated above.

```{r}
levels_binned_weighted <- levels_binned %>% 
  group_by(pres_bin, date_bin, file, n_prof) %>% 
  mutate(profile_weight = 1 / n()) %>% 
  ungroup() %>% 
  left_join(profile_weighted, by = "file") %>% 
  mutate(weight = profile_weight * distance_weight) %>% 
  group_by(pres_bin, date_bin) %>% 
  mutate(weight = weight / sum(weight)) %>% 
  ungroup() %>% 
  select(pres_bin, date_bin, weight, file, date, pres, temp, everything())

levels_binned_weighted
```

The most straightforward way to aggregate is using `weighted.mean()`, which is a good way to get a preliminary view of our results:

```{r}
levels_aggregated <- levels_binned_weighted %>% 
  group_by(date_bin, pres_bin) %>% 
  filter(sum(is.finite(temp)) > 1) %>% 
  summarise(
    temp_mean = weighted.mean(temp, w = weight, na.rm = TRUE)
  ) %>% 
  ungroup()
```
```{r}
ggplot(levels_aggregated, aes(x = temp_mean, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)), 
    col = "grey80",
    orientation = "y", 
    data = levels_binned
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(date_bin))
```

This is a good first step in your analysis: if the above diagram was not in line with our knowledge of the ocean at this location, it is a clue that some part of our analysis went wrong. However, it is not able to communicate the spread of values that we observed in each bin. Are the profiles for a given month of the year similar over the last 10 years or not? 

One technique that allows calculation of error in this way is sampling: we can draw a random sample of size `n` from the values available for each bin (applying the weights such that values with a higher weight are more likely to be drawn than others). We can repeat this analysis `k` times and use the distribution of the values we observe for each bin to communicate the uncertainty of our results.

```{r}
set.seed(3948)

levels_aggregated_randomized <- levels_binned_weighted %>%
  crossing(tibble(sample_number = 1:100)) %>%
  group_by(sample_number, date_bin, pres_bin) %>%
  sample_n(size = n(), replace = TRUE) %>% 
  summarise(
    temp_weighted_mean = weighted.mean(temp, w = weight, na.rm = TRUE),
  ) %>%
  group_by(pres_bin, date_bin) %>% 
  summarise(
    temp_weighted_mean_q05 = quantile(temp_weighted_mean, 0.05, na.rm = TRUE),
    temp_weighted_mean_q50 = median(temp_weighted_mean, na.rm = TRUE),
    temp_weighted_mean_q95 = quantile(temp_weighted_mean, 0.95, na.rm = TRUE)
  ) %>% 
  ungroup()
```

```{r}
ggplot(levels_aggregated_randomized, aes(x = temp_weighted_mean_q50, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)), 
    col = "grey80",
    orientation = "y", 
    data = levels_binned
  ) +
  geom_ribbon(
    aes(xmin = temp_weighted_mean_q05, xmax = temp_weighted_mean_q95),
    alpha = 0.5
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(date_bin))
```

In this case the weighted mean and randomized approach give the same results. This is a good thing! Another thing to note is that in the above example the weighted mean varies little even with the bootstrapping approach (the `geom_ribbon()` behind each profile is barely if at all visible).

### Kriging

(subsurface geology folks are all over this kind of thing but I have to look up what the best way to go about doing it is)

## Sections

Computing a section based on a collection of Argo profiles is similar to computing an average profile from a single point except we have an added dimension along which we want to bin the profiles. 

As an example, we'll look at a cross-section of the Gulf stream off Nova Scotia. The first step is to pick a line that defines your section. The calculations are easier when your section can be defined by exactly two points, but you can use a section with an arbitrary number of points. Because we'll be using the [s2 package](https://r-spatial.github.io/s2/) for our calculations, I'll also construct an s2 geography for the start point, end point, and the line between them.

```{r}
library(s2)

section_points_lon <- c(-62.0, -55.9)
section_points_lat <- c(42.0, 35.9)

section_start <- s2_geog_point(section_points_lon[1], section_points_lat[1])
section_end <- s2_geog_point(section_points_lon[2], section_points_lat[2])
section_line <- s2_make_line(section_points_lon, section_points_lat)
```

We can use the line to subset profiles based on the criteria we discussed above, this time adding in `s2::s2_dwithin()` to find profiles within a specified distance of our profile line. I've chosen a threshold of 100 km in this case. Because s2 package functions work on s2_geography objects, I've kept the geography vector representing the point where the profile was collected so that we can use it later without recalculating.

```{r}
section_profiles <- argo_global_prof() %>%
  argo_filter_data_mode("delayed") %>% 
  argo_filter_date(
    date_min = mooring_start,
    date_max = mooring_end
  ) %>% 
  mutate(geog = s2_geog_point(longitude, latitude)) %>% 
  filter(s2_dwithin(geog, section_line, 100 * 1000))

section_profiles
```

Similar to the profile calculation, we need to calculate some information about the suitability of each profile to our section. In addition to the distance from the section line, we will also need a measure of how far along the section line each profile is so that we can bin it accordingly. I also add a filter here to remove profiles that are "off the end" of the section (whose closest point *is* the start or end point). This is where the two-point section definition makes our life easier: distance along the profile is just the distance from the start point. If you really need a section defined by multiple points you will need to use a projection and the linear referencing function `geos::geos_project()`.

```{r}
section_profile_weighted <- section_profiles %>% 
  mutate(
    point_on_section = s2_closest_point(section_line, geog),
    distance_km = s2_distance(section_line, geog) / 1000,
    distance_along_section_km = s2_distance(
      point_on_section, 
      section_start
    ) / 1000,
    distance_weight = 1 / sqrt(distance_km)
  ) %>% 
  filter(
    !s2_equals(point_on_section, section_start),
    !s2_equals(point_on_section, section_end)
  ) %>% 
  select(
    file, geog, point_on_section,
    distance_km, distance_along_section_km, distance_weight
  )
```

Now is a good time to plot our section to make sure that our math was correct!

```{r}
ggplot() +
  layer_spatial(
    sf::st_segmentize(sf::st_as_sfc(section_line), 10000),
    size = 1
  ) +
  layer_spatial(
    s2_minimum_clearance_line_between(
      section_line, 
      section_profile_weighted$geog
    ),
    col = "grey40"
  ) +
  layer_spatial(section_profile_weighted$geog)
```

From here we have enough information to load our levels.

```{r}
section_levels <- argo_prof_levels(section_profile_weighted)
```

Again, the first step once the levels are loaded is to check which QC flag values we can use to ensure reasonable values in our result. Because we're working with delayed mode data, there is a chance that the `temp_adjusted` column contains higher quality results. I've plotted both below.

```{r}
ggplot(section_levels, aes(y = pres, x = temp, col = temp_qc)) +
  geom_point() +
  scale_y_reverse()

ggplot(section_levels, aes(y = pres, x = temp_adjusted, col = temp_adjusted_qc)) +
  geom_point() +
  scale_y_reverse()
```

In many cases the adjusted values are identical to the non-adjusted values, but it is reasonable to use the adjusted values where they have been made available. Some adjusted data has been marked as not "good data", however, which we need to censor before "preferring" the adjusted value over the non-adjusted one.

```{r}
section_levels_clean <- section_levels %>% 
  argo_qc_censor_if_not(
    c(temp, pres, temp_adjusted, pres_adjusted),
    qc_flag = 1
  ) %>% 
  argo_use_adjusted(c(temp, pres))
```

As above, we can remove profiles that contain few temperature values to simplify the upcoming analysis.

```{r}
section_levels_clean <- section_levels_clean %>% 
  group_by(file, n_prof) %>% 
  filter(mean(is.finite(temp)) > 0.5) %>% 
  ungroup()
```

Finally, we can visualize all the profiles to make sure our profiles are composed of reasonable values.

```{r}
ggplot(section_levels_clean, aes(y = pres, x = temp)) +
  geom_line(
    aes(group = interaction(file, n_prof)),
    orientation = "y",
    alpha = 0.02
  ) +
  scale_y_reverse()
```

Now we're ready to bin! While it is also valid to bin using a `date_bin` if enough profiles are available, in this case I'm going to demonstrate binning along the transect such that there are still two bin dimensions. When calculating uncertainty, this should show up in our results for the top pressure bins whose temperature responds most readily to seasonal fluctuations.

```{r}
section_levels_binned <- section_levels_clean %>%
  left_join(section_profile_weighted, by = "file") %>% 
  mutate(
    pres_bin = floor(pres / 100) * 100 + 50,
    distance_bin = floor(distance_along_section_km / 40) * 40 + 20
  )

section_levels_binned
```

As above, we can aggregate using `weighted.mean()` to get a preliminary view of our results:

```{r}
section_levels_aggregated <- section_levels_binned %>% 
  group_by(distance_bin, pres_bin) %>% 
  filter(sum(is.finite(temp)) > 1) %>% 
  summarise(
    temp_mean = weighted.mean(temp, w = distance_weight, na.rm = TRUE)
  ) %>% 
  ungroup()

section_levels_aggregated
```

There are too many bins to examine all of them for quality. Instead, I'll examine a subset of them to make sure our `weighted.mean()` calculation didn't create unrealistic estimates.

```{r}
test_distance_bins <- c(20, 320, 620, 820)

section_levels_aggregated %>% 
  filter(distance_bin %in% test_distance_bins) %>% 
  ggplot(aes(x = temp_mean, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)),
    col = "grey80",
    orientation = "y",
    data = section_levels_binned %>% 
      filter(distance_bin %in% test_distance_bins)
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(distance_bin))
```

Finally, we can visualize all of the bins together!

```{r}
ggplot(
  section_levels_aggregated,
  aes(x = distance_bin, y = pres_bin, fill = temp_mean)
) +
  geom_raster() +
  scale_fill_viridis_c() +
  scale_y_reverse()
```
